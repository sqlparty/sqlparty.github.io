---
layout: post
status: publish
published: true
title: HDFS环境搭建-NameNode HA搭建实录
author:
  display_name: super
  login: super
  email: sqlparty@gmail.com
  url: ''
author_login: super
author_email: sqlparty@gmail.com
excerpt: "了解了<a title=\"HDFS环境搭建-NameNode HA原理与基本配置\" href=\"http:&#47;&#47;www.sqlparty.com&#47;hdfs%e7%8e%af%e5%a2%83%e6%90%ad%e5%bb%ba-namenode-ha%e5%8e%9f%e7%90%86%e4%b8%8e%e5%9f%ba%e6%9c%ac%e9%85%8d%e7%bd%ae&#47;\"
  target=\"_blank\">NameNode HA原理与基本配置<&#47;a>后，我们尝试进行NameNode HA搭建。\r\n<h2>1.规划<&#47;h2>\r\n使用三台主机h3,h4,h5进行测试，每台主机上搭建的组件如下：\r\n<ul>\r\n\t<li>h3:
  &nbsp; (Active)NameNode+ZKFC+JournalNode+DataNode<&#47;li>\r\n\t<li>h4: &nbsp; (StandBy)NameNode+ZKFC+JournalNode+DataNode<&#47;li>\r\n\t<li>h5:&nbsp;&nbsp;
  JournalNode+DataNode<&#47;li>\r\n<&#47;ul>\r\nZKFC使用已经搭建好的ZooKeeper集群，节点包括CH22,CH34,CH35。\r\n\r\n以下all-shell表示在h3,h4,h5三台主机上都需要执行，h3-shell表示只在h3上执行，类推。\r\n\r\n"
wordpress_id: 732
wordpress_url: http://www.sqlparty.com/?p=732
date: '2013-10-12 12:01:51 +0800'
date_gmt: '2013-10-12 04:01:51 +0800'
categories:
- 大数据
tags:
- Hadoop
- hdfs
- ha
---
<p>了解了<a title="HDFS环境搭建-NameNode HA原理与基本配置" href="http:&#47;&#47;www.sqlparty.com&#47;hdfs%e7%8e%af%e5%a2%83%e6%90%ad%e5%bb%ba-namenode-ha%e5%8e%9f%e7%90%86%e4%b8%8e%e5%9f%ba%e6%9c%ac%e9%85%8d%e7%bd%ae&#47;" target="_blank">NameNode HA原理与基本配置<&#47;a>后，我们尝试进行NameNode HA搭建。</p>
<h2>1.规划<&#47;h2><br />
使用三台主机h3,h4,h5进行测试，每台主机上搭建的组件如下：</p>
<ul>
<li>h3: &nbsp; (Active)NameNode+ZKFC+JournalNode+DataNode<&#47;li>
<li>h4: &nbsp; (StandBy)NameNode+ZKFC+JournalNode+DataNode<&#47;li>
<li>h5:&nbsp;&nbsp; JournalNode+DataNode<&#47;li><br />
<&#47;ul><br />
ZKFC使用已经搭建好的ZooKeeper集群，节点包括CH22,CH34,CH35。</p>
<p>以下all-shell表示在h3,h4,h5三台主机上都需要执行，h3-shell表示只在h3上执行，类推。</p>
<p><!--more--></p>
<h2>2.安装<&#47;h2><br />
安装基于本地搭建的Yum仓库，其搭建与使用见文章&ldquo;<a title="创建cdh4本地Yum仓库" href="http:&#47;&#47;www.sqlparty.com&#47;%e5%88%9b%e5%bb%bacdh4%e6%9c%ac%e5%9c%b0yum%e4%bb%93%e5%ba%93&#47;" target="_blank">搭建Yum仓库<&#47;a>&rdquo;。</p>
<p><strong>JAVA安装：<&#47;strong></p>
<p>下载后上传至服务器&#47;db&#47;soft目录</p>
<p><span style="color: #0000ff;">all-shell>chmod u+x jdk-6u35-linux-i586-rpm.bin<&#47;span><br />
<span style="color: #0000ff;">all-shell>.&#47;jdk-6u35-linux-i586-rpm.bin<&#47;span></p>
<p>正常结束就完成JDK安装。接下来配置JAVA_HOME。</p>
<p><span style="color: #0000ff;">all-shell>vi &#47;etc&#47;profile<&#47;span><br />
<span style="color: #0000ff;">...<&#47;span><br />
<span style="color: #0000ff;">export JAVA_HOME=&#47;usr&#47;java&#47;jdk1.6.0_35<&#47;span><br />
<span style="color: #0000ff;">export JAVA_BIN=$JAVA_HOME&#47;bin<&#47;span><br />
<span style="color: #0000ff;">export CLASSPATH=.:$JAVA_HOME&#47;lib&#47;dt.jar<&#47;span><br />
<span style="color: #0000ff;">export PATH=$JAVA_BIN:$PATH<&#47;span><br />
<span style="color: #0000ff;">all-shell>. &#47;etc&#47;profile<&#47;span>&nbsp; #使本连接生效<br />
<span style="color: #0000ff;">all-shell>java -version<&#47;span><br />
<span style="color: #0000ff;">java version "1.6.0_35"<&#47;span><br />
<span style="color: #0000ff;">Java(TM) SE Runtime Environment (build 1.6.0_35-b10)<&#47;span><br />
<span style="color: #0000ff;">Java HotSpot(TM) Client VM (build 20.10-b01, mixed mode, sharing)<&#47;span></p>
<p>说明配置成功。</p>
<p><strong>h3,h4,h5三台主机，分别如下安装：<&#47;strong></p>
<p><span style="color: #0000ff;">h3&#47;h4-shell> yum -y install hadoop-hdfs-namenode <&#47;span><br />
<span style="color: #0000ff;">h3&#47;h4-shell> yum -y install hadoop-hdfs-zkfc<&#47;span><br />
<span style="color: #0000ff;">h3&#47;h4-shell> yum -y install hadoop-hdfs-journalnode<&#47;span><br />
<span style="color: #0000ff;">h3&#47;h4-shell> yum -y install hadoop-hdfs-datanode<&#47;span></p>
<p><span style="color: #0000ff;">h5-shell> yum -y install hadoop-hdfs-journalnode<&#47;span><br />
<span style="color: #0000ff;">h5-shell> yum -y install hadoop-hdfs-datanode<&#47;span></p>
<h2>3.准备<&#47;h2></p>
<h3>3.1 Linux系统层面修改<&#47;h3><br />
关闭iptables，由于涉及到多个端口的开启，方便而已。为安全，可以分别开启各端口以便交互访问。</p>
<p><span style="color: #0000ff;">all-shell> service iptables stop<&#47;span><br />
<span style="color: #0000ff;">all-shell> chkconfig --level 2345 iptables off<&#47;span></p>
<p>关闭SELinux，主要是RHEL 6下SELinux启用时不允许免密码登录，影响fencing机制。</p>
<p><span style="color: #0000ff;">all-shell> setenforce 0<&#47;span><br />
<span style="color: #0000ff;">all-shell> sed -i 's&#47;SELINUX=enforcing&#47;SELINUX=disabled&#47;' &#47;etc&#47;selinux&#47;config<&#47;span></p>
<h3>3.2 修改&#47;etc&#47;hosts，添加涉及到的Hadoop节点主机名称<&#47;h3><br />
<span style="color: #0000ff;">all-shell> vi &#47;etc&#47;hosts<&#47;span><br />
<span style="color: #0000ff;">...<&#47;span><br />
<span style="color: #0000ff;">192.168.10.22 CH22<&#47;span><br />
<span style="color: #0000ff;">192.168.10.34 CH34<&#47;span><br />
<span style="color: #0000ff;">192.168.10.35 CH35<&#47;span><br />
<span style="color: #0000ff;">192.168.2.4 h4<&#47;span><br />
<span style="color: #0000ff;">192.168.2.3 h3<&#47;span><br />
<span style="color: #0000ff;">192.168.2.5 h5<&#47;span></p>
<h3>3.3 创建存储目录<&#47;h3><br />
<span style="color: #0000ff;">h3&#47;h4-shell> mkdir -p &#47;data&#47;nn &#47;data&#47;dn&#47;&nbsp; &#47;data&#47;jn&nbsp; &#47;data&#47;zk<&#47;span><br />
<span style="color: #0000ff;">h5-shell>mkdir -p &#47;data&#47;dn&#47;&nbsp; &#47;data&#47;jn<&#47;span><br />
<span style="color: #0000ff;">all-shell> chown -R hdfs:hadoop &#47;data<&#47;span><br />
<span style="color: #0000ff;">all-shell> chmod 770 &#47;data<&#47;span></p>
<h2>4.配置<&#47;h2><br />
注：如果遇到&#47;etc&#47;hadoop&#47;conf目录不存在，你就会注意到&#47;etc&#47;hadoop&#47;conf其实是一个软链接，其指向&#47;etc&#47;alternatives&#47;hadoop-conf，而&#47;etc&#47;alternatives&#47;hadoop-conf又是一个软链接，指向&#47;etc&#47;hadoop&#47;conf.mycluster，而后者不存在。可以通过拷贝&#47;etc&#47;hadoop&#47;目录下的实际存在的目录的方式来生成这个目录，这样这个目录是完整自己控制的目录，不会受到升级等的影响。如： cp -R &#47;etc&#47;hadoop&#47;conf.empty &#47;etc&#47;hadoop&#47;conf.mycluster。这样就能够访问&#47;etc&#47;hadoop&#47;conf目录了。</p>
<p><span style="color: #0000ff;">all-shell> vi &#47;etc&#47;hadoop&#47;conf&#47;core-site.xml<&#47;span><br />
<span style="color: #0000ff;"><?xml version="1.0"?><&#47;span><br />
<span style="color: #0000ff;"><?xml-stylesheet type="text&#47;xsl" href="configuration.xsl"?><&#47;span><br />
<span style="color: #0000ff;"><configuration><&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>fs.defaultFS<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>hdfs:&#47;&#47;hatest<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;<&#47;configuration><&#47;span></p>
<p>h3和h4上安装NameNode+ZKFC+JournalNode+DataNode。</p>
<p><span style="color: #0000ff;">h3&#47;h4-shell> vi &#47;etc&#47;hadoop&#47;conf&#47;hdfs-site.xml<&#47;span><br />
<span style="color: #0000ff;"><?xml version="1.0"?><&#47;span><br />
<span style="color: #0000ff;"><?xml-stylesheet type="text&#47;xsl" href="configuration.xsl"?><&#47;span><br />
<span style="color: #0000ff;"><configuration><&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.permissions.superusergroup<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>hdfs<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;"><!-- For NameNode --><&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.namenode.name.dir<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>file:&#47;&#47;&#47;data&#47;nn<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;"><!-- For DataNode --><&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.datanode.data.dir<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>file:&#47;&#47;&#47;data&#47;dn<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;"><!-- HA --><&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.nameservices<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>hatest<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.ha.namenodes.hatest<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>nn1,nn2<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;<&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.namenode.rpc-address.hatest.nn1<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>h3:8020<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.namenode.rpc-address.hatest.nn2<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>h4:8020<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.namenode.http-address.hatest.nn1<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>h3:50070<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.namenode.http-address.hatest.nn2<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>h4:50070<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.client.failover.proxy.provider.hatest<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;"><!-- For Fence --><&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.ha.fencing.methods<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>sshfence<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.ha.fencing.ssh.private-key-files<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>&#47;var&#47;lib&#47;hadoop-hdfs&#47;.ssh&#47;id_rsa<&#47;value><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <description>The SSH private key files to use with the builtin sshfence fencer<&#47;description><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;"><!-- For JournalNode --><&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.namenode.shared.edits.dir<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>qjournal:&#47;&#47;h3:8485;h4:8485;h5:8485&#47;hatest<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.journalnode.edits.dir<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>&#47;data&#47;jn<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;"><!-- For autofailover, ZKFC --><&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.ha.automatic-failover.enabled<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>true<&#47;value><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <description>Whether automatic failover is enabled. See the HDFS High Availability documentation for details on automatic HA configuration<&#47;description><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>ha.zookeeper.quorum<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>CH22:2281,CH34:2281,CH35:2281<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span><br />
<span style="color: #0000ff;"><&#47;configuration><&#47;span></p>
<p>而h5上只需要安装JournalNode和DataNode，以及访问NameNode，其实只需要配置如下：</p>
<p><span style="color: #0000ff;">h5-shell> vi &#47;etc&#47;hadoop&#47;conf&#47;hdfs-site.xml<&#47;span><br />
<span style="color: #0000ff;"><?xml version="1.0"?><&#47;span><br />
<span style="color: #0000ff;"><?xml-stylesheet type="text&#47;xsl" href="configuration.xsl"?><&#47;span><br />
<span style="color: #0000ff;"><configuration><&#47;span><br />
<span style="color: #0000ff;"><!-- For DataNode --><&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.datanode.data.dir<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>file:&#47;&#47;&#47;data&#47;dn<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;"><!-- For Access NameNode --><&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.nameservices<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>hatest<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.ha.namenodes.hatest<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>nn1,nn2<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;<&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.namenode.rpc-address.hatest.nn1<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>h3:8020<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.namenode.rpc-address.hatest.nn2<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>h4:8020<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.namenode.http-address.hatest.nn1<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>h3:50070<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.namenode.http-address.hatest.nn2<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>h4:50070<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.client.failover.proxy.provider.hatest<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;"><!-- For JournalNode --><&#47;span><br />
<span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.namenode.shared.edits.dir<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>qjournal:&#47;&#47;h3:8485;h4:8485;h5:8485&#47;hatest<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span></p>
<p><span style="color: #0000ff;">
<property><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <name>dfs.journalnode.edits.dir<&#47;name><&#47;span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <value>&#47;data&#47;jn<&#47;value><&#47;span><br />
<span style="color: #0000ff;"><&#47;property><&#47;span><br />
<span style="color: #0000ff;"><&#47;configuration><&#47;span></p>
<h2>5.配置NameNode之间的sshfence<&#47;h2><br />
<span style="color: #0000ff;">h3&#47;h4-shell> sudo -u hdfs ssh-keygen<&#47;span><br />
<span style="color: #0000ff;">Generating public&#47;private rsa key pair.<&#47;span><br />
<span style="color: #0000ff;">Enter file in which to save the key (&#47;var&#47;lib&#47;hadoop-hdfs&#47;.ssh&#47;id_rsa):&nbsp; <span style="color: #000000;">#直接回车<&#47;span><&#47;span><br />
<span style="color: #0000ff;">Enter passphrase (empty for no passphrase):&nbsp; <span style="color: #000000;">#直接回车，必须配置成无密码的方式<&#47;span><&#47;span><br />
<span style="color: #0000ff;">Enter same passphrase again:&nbsp;&nbsp;&nbsp;<span style="color: #000000;"> #直接回车<&#47;span><&#47;span><br />
<span style="color: #0000ff;">Your identification has been saved in &#47;var&#47;lib&#47;hadoop-hdfs&#47;.ssh&#47;id_rsa.<&#47;span><br />
<span style="color: #0000ff;">Your public key has been saved in &#47;var&#47;lib&#47;hadoop-hdfs&#47;.ssh&#47;id_rsa.pub.<&#47;span><br />
<span style="color: #0000ff;">The key fingerprint is:<&#47;span><br />
<span style="color: #0000ff;">7d:ad:7b:5a:53:b1:ee:53:94:f5:f7:cd:4e:c9:75:bb hdfs@h3<&#47;span><br />
<span style="color: #0000ff;">The key's randomart image is:<&#47;span><br />
<span style="color: #0000ff;">+--[ RSA 2048]----+<&#47;span><br />
<span style="color: #0000ff;">|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |<&#47;span><br />
<span style="color: #0000ff;">|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .|<&#47;span><br />
<span style="color: #0000ff;">|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .+|<&#47;span><br />
<span style="color: #0000ff;">|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; .&nbsp;&nbsp; . .O|<&#47;span><br />
<span style="color: #0000ff;">|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; S . . o+X|<&#47;span><br />
<span style="color: #0000ff;">|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; . ..=*|<&#47;span><br />
<span style="color: #0000ff;">|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; . o+o|<&#47;span><br />
<span style="color: #0000ff;">|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ooE.|<&#47;span><br />
<span style="color: #0000ff;">|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; oo ..|<&#47;span><br />
<span style="color: #0000ff;">+-----------------+<&#47;span></p>
<p>以下将自己的公钥拷贝给对方，以便实现自己免密码登录对方。</p>
<p><span style="color: #0000ff;">h3-shell> sudo -u hdfs ssh-copy-id -i &#47;var&#47;lib&#47;hadoop-hdfs&#47;.ssh&#47;id_rsa.pub hdfs@h4<&#47;span><br />
<span style="color: #0000ff;">h4-shell> sudo -u hdfs ssh-copy-id -i &#47;var&#47;lib&#47;hadoop-hdfs&#47;.ssh&#47;id_rsa.pub hdfs@h3<&#47;span></p>
<p>测试免密码登录：</p>
<p><span style="color: #0000ff;">h3-shell> sudo -u hdfs ssh hdfs@h4<&#47;span></p>
<p>能够直接登录，则说明成功！h4登录h3同理。</p>
<h2>6.启动HDFS<&#47;h2></p>
<h3>6.1启动Journal Node<&#47;h3><br />
NameNode启动时会连接Journal Node：</p>
<p><span style="color: #0000ff;">all-shell> service hadoop-hdfs-journalnode start<&#47;span></p>
<h3>6.2格式化h3上的NameNode<&#47;h3><br />
h3上的NameNode规划为Active NameNode，称之为主NN。这样h4上的NN称为从NN。</p>
<p><span style="color: #0000ff;">h3-shell> sudo -u hdfs hadoop namenode -format<&#47;span></p>
<h3>6.3启动主NN<&#47;h3><br />
<span style="color: #0000ff;">h3-shelll> service hadoop-hdfs-namenode start<&#47;span></p>
<h3>6.4同步从NN<&#47;h3><br />
<span style="color: #0000ff;">h4-shell> sudo -u hdfs hadoop namenode -bootstrapStandby<&#47;span></p>
<p><em>如果同步数据时遇到如下问题：<&#47;em><br />
<em>************************************************************&#47;<&#47;em><br />
<em>13&#47;10&#47;10 18:24:20 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]<&#47;em><br />
<em>=====================================================<&#47;em><br />
<em>About to bootstrap Standby ID nn2 from:<&#47;em><br />
<em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Nameservice ID: hatest<&#47;em><br />
<em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Other Namenode ID: nn1<&#47;em><br />
<em>&nbsp; Other NN's HTTP address: h3:50070<&#47;em><br />
<em>&nbsp; Other NN's IPC&nbsp; address: h3&#47;192.168.2.3:8020<&#47;em><br />
<em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Namespace ID: 763881322<&#47;em><br />
<em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Block pool ID: BP-602901281-192.168.2.3-1381397584877<&#47;em><br />
<em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cluster ID: CID-823445da-c230-4461-aba1-18504e7e3f46<&#47;em><br />
<em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Layout version: -40<&#47;em><br />
<em>=====================================================<&#47;em><br />
<em>Re-format filesystem in Storage Directory &#47;data&#47;nn ? (Y or N) Y<&#47;em><br />
<em>13&#47;10&#47;10 18:24:28 INFO namenode.NNStorage: Storage directory &#47;data&#47;nn has been successfully formatted.<&#47;em><br />
<em>13&#47;10&#47;10 18:24:28 <span style="color: #ff0000;">FATAL ha.BootstrapStandby: Unable to read transaction ids 1-25 from the configured shared edits storage qjournal:&#47;&#47;h3:8485;h4:8485;h5:8485&#47;hatest. Please copy these logs into the shared edits storage or call saveNamespace on the active node.<&#47;span><&#47;em><br />
<em><span style="color: #ff0000;">Error: Gap in transactions. Expected to be able to read up until at least txid 25 but unable to find any edit logs containing txid 2<&#47;span><&#47;em><br />
<em>13&#47;10&#47;10 18:24:28 INFO util.ExitUtil: Exiting with status 6<&#47;em><br />
<em>13&#47;10&#47;10 18:24:28 INFO namenode.NameNode: SHUTDOWN_MSG:<&#47;em><br />
<em>&#47;************************************************************<&#47;em><br />
<em>SHUTDOWN_MSG: Shutting down NameNode at h4&#47;192.168.2.4<&#47;em><br />
<em>************************************************************&#47;<&#47;em></p>
<p><em>则似乎是一个已知的问题，参考：<a href="http:&#47;&#47;grokbase.com&#47;t&#47;cloudera&#47;cdh-user&#47;12art3v1jn&#47;namenode-bootstrapstandby-failed-always" target="_blank">"namenode -bootstrapStandby" failed always<&#47;a><&#47;em></p>
<p><em>解决方案：<&#47;em></p>
<p><em>直接拷贝主节点（活跃NameNode）的NameNode目录下的全部数据，到从节点的NameNode目录下即可，如这里：<&#47;em><br />
<em><span style="color: #0000ff;">h3-shell> cd &#47;data&#47;nn<&#47;span><&#47;em><br />
<em><span style="color: #0000ff;">h3-shell> rsync -av * root@h4:&#47;data&#47;nn&#47;<&#47;span><&#47;em><br />
<em>这样，继续往下处理即可。<&#47;em></p>
<h3>6.5启动从NN<&#47;h3><br />
<span style="color: #0000ff;">h4-shell> service hadoop-hdfs-namenode start<&#47;span></p>
<h3><span style="color: #000000;">6.6配置自动切换<&#47;span><&#47;h3><br />
在任意一个NameNode上运行即可。会创建一个znode用于自动故障转移。</p>
<p><span style="color: #0000ff;">h3-shell> hdfs zkfc -formatZK<&#47;span></p>
<p>启动zkfc</p>
<p><span style="color: #0000ff;">h3&#47;h4-shell> service hadoop-hdfs-zkfc start<&#47;span></p>
<h3>6.7启动DataNode<&#47;h3><br />
<span style="color: #0000ff;">all-shell> service hadoop-hdfs-datanode start<&#47;span></p>
<p>如上所有操作完成后，各节点上Hadoop组件运行情况：</p>
<p><span style="color: #0000ff;">h3-shell> jps<&#47;span><br />
<span style="color: #0000ff;">5003 DFSZKFailoverController<&#47;span><br />
<span style="color: #0000ff;">30612 DataNode<&#47;span><br />
<span style="color: #0000ff;">5629 Jps<&#47;span><br />
<span style="color: #0000ff;">5548 NameNode<&#47;span><br />
<span style="color: #0000ff;">30289 JournalNode<&#47;span></p>
<p><span style="color: #0000ff;">h4-shell> jps<&#47;span><br />
<span style="color: #0000ff;">5708 NameNode<&#47;span><br />
<span style="color: #0000ff;">1801 JournalNode<&#47;span><br />
<span style="color: #0000ff;">6275 Jps<&#47;span><br />
<span style="color: #0000ff;">2393 DataNode<&#47;span><br />
<span style="color: #0000ff;">5817 DFSZKFailoverController<&#47;span></p>
<p><span style="color: #0000ff;">h5-shell> jps<&#47;span><br />
<span style="color: #0000ff;">1163 Jps<&#47;span><br />
<span style="color: #0000ff;">1825 JournalNode<&#47;span><br />
<span style="color: #0000ff;">2654 DataNode<&#47;span></p>
<p>web访问 http:&#47;&#47;h3:50070&#47;，可以看到：</p>
<p><a href="http:&#47;&#47;www.sqlparty.com&#47;wp-content&#47;uploads&#47;2013&#47;10&#47;NameNode_HA_Web.png"><img class="alignnone size-large wp-image-733" alt="NameNode_HA_Web" src="http:&#47;&#47;www.sqlparty.com&#47;wp-content&#47;uploads&#47;2013&#47;10&#47;NameNode_HA_Web-710x1024.png" width="710" height="1024" &#47;><&#47;a></p>
<h2>7.HDFS启动后<&#47;h2></p>
<h3>7.1 &#47;tmp目录<&#47;h3><br />
很多Hadoop生态圈中的应用都期望HDFS中有一个&#47;tmp目录，功能与Linux下的&#47;tmp目录类似，需要所有用户可读写。应主动创建HDFS &#47;tmp目录。这个目录如果不主动创建，某些进程会自动创建，但是其权限控制可能会使得其他应用不能正常使用它。</p>
<p><span style="color: #0000ff;">h5-shell>hadoop fs -ls &#47;<&#47;span><br />
<span style="color: #0000ff;">h5-shell>sudo -u hdfs hadoop fs -mkdir &#47;tmp<&#47;span><br />
<span style="color: #0000ff;">h5-shell>sudo -u hdfs hadoop fs -chmod -R 1777 &#47;tmp<&#47;span><br />
<span style="color: #0000ff;">h5-shell>hadoop fs -ls &#47;<&#47;span><br />
<span style="color: #0000ff;">Found 1 items<&#47;span><br />
<span style="color: #0000ff;">drwxrwxrwt&nbsp;&nbsp; - hdfs hdfs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 2013-10-10 23:14 &#47;tmp<&#47;span></p>
<h3>7.2 HDFS功能测试<&#47;h3><br />
<span style="color: #0000ff;">h5-shell> hadoop fs -put network.sh &#47;tmp&#47;<&#47;span><br />
<span style="color: #0000ff;">h5-shell> hadoop fs -mv &#47;tmp&#47;network.sh &#47;tmp&#47;to_del.sh<&#47;span><br />
<span style="color: #0000ff;">h5-shell> hadoop fs -ls &#47;tmp<&#47;span><br />
<span style="color: #0000ff;">Found 1 items<&#47;span><br />
<span style="color: #0000ff;">-rw-r--r--&nbsp;&nbsp; 3 root hdfs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1044 2013-10-10 23:16 &#47;tmp&#47;to_del.sh<&#47;span><br />
<span style="color: #0000ff;">h5-shell> hadoop fs -rm &#47;tmp&#47;to_del.sh<&#47;span><br />
<span style="color: #0000ff;">Deleted &#47;tmp&#47;to_del.sh<&#47;span></p>
<h3>7.3 NameNode HA自动切换测试<&#47;h3><br />
通过web，我们可以确定h3是Active NameNode，h4是StandBy NameNode。</p>
<p><a href="http:&#47;&#47;www.sqlparty.com&#47;wp-content&#47;uploads&#47;2013&#47;10&#47;h4_standby.png"><img class="alignnone size-full wp-image-734" alt="h4_standby" src="http:&#47;&#47;www.sqlparty.com&#47;wp-content&#47;uploads&#47;2013&#47;10&#47;h4_standby.png" width="507" height="78" &#47;><&#47;a></p>
<p>这里测试自动切换机制是否有效，强行关闭Active NameNode</p>
<p><span style="color: #0000ff;">h3-shell> kill -9 5548<&#47;span></p>
<p>通过web访问 http:&#47;&#47;h4:50070&#47;，可以看到短暂时间内（可能是几秒）完成了切换：</p>
<p><a href="http:&#47;&#47;www.sqlparty.com&#47;wp-content&#47;uploads&#47;2013&#47;10&#47;h4_active.png"><img class="alignnone size-full wp-image-735" alt="h4_active" src="http:&#47;&#47;www.sqlparty.com&#47;wp-content&#47;uploads&#47;2013&#47;10&#47;h4_active.png" width="493" height="68" &#47;><&#47;a></p>
<p>而访问可以正常进行：</p>
<p><span style="color: #0000ff;">h5-shell>&nbsp; hadoop fs -ls &#47;<&#47;span><br />
<span style="color: #0000ff;">Found 1 items<&#47;span><br />
<span style="color: #0000ff;">drwxrwxrwt&nbsp;&nbsp; - hdfs hdfs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 2013-10-10 23:17 &#47;tmp<&#47;span></p>
<h3>7.4管理命令测试<&#47;h3><br />
在h3或者h4上都可以执行手动切换</p>
<p><span style="color: #0000ff;">h3-shell> sudo -u hdfs hdfs haadmin -failover nn1 nn2<&#47;span></p>
<p>查看某Namenode的状态</p>
<p><span style="color: #0000ff;">h3-shell> sudo -u hdfs hdfs haadmin -getServiceState nn2<&#47;span><br />
<span style="color: #0000ff;">active<&#47;span><br />
<span style="color: #0000ff;">h3-shell> sudo -u hdfs hdfs haadmin -getServiceState nn1<&#47;span><br />
<span style="color: #0000ff;">standby<&#47;span></p>
<h2>8.小结<&#47;h2><br />
在搭建过程中，是对原理不断验证和加强理解的过程，中间很容易出现问题，保持清醒头脑，并经常去回顾对原理的认识，复杂的过程也变得逐步清晰了，Done!</p>
<p>参考：<br />
<a href="http:&#47;&#47;hadoop.apache.org&#47;docs&#47;current&#47;hadoop-project-dist&#47;hadoop-common&#47;core-default.xml" target="_blank">http:&#47;&#47;hadoop.apache.org&#47;docs&#47;current&#47;hadoop-project-dist&#47;hadoop-common&#47;core-default.xml<&#47;a></p>
