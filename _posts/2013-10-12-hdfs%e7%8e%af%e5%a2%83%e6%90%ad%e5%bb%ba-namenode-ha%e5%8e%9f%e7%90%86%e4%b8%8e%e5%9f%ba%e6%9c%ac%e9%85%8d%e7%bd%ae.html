---
layout: post
status: publish
published: true
title: HDFS环境搭建-NameNode HA原理与基本配置
author:
  display_name: super
  login: super
  email: sqlparty@gmail.com
  url: ''
author_login: super
author_email: sqlparty@gmail.com
excerpt: "CDH4以后，支持NameNode做HA，这样避免了单点故障(single point of failure, SPOF)的问题，保障了整个集群的整体可用性。\r\n<h2>原理</h2>\r\n在HA出现之前，Secondary
  NameNode看上去似乎起着保证NameNode高可用的作用，但其实不然，Secondary NameNode只是定期合并fsimage和edits信息，替NameNode整理edits。它不是实时与NameNode保持更新的，所以用它来顶替宕机的NameNode势必会有元数据丢失风险！\r\n\r\nNameNode的HA方案，是一个集群中配置两个（当前不能配置多个）NameNode，其中一个处于活跃Active状态，另一个为Standby状态。如果活跃的NameNode宕机或者正常关闭，Standby可以快速切换为Active。\r\n\r\nNameNode
  HA的架构图：\r\n\r\n<a href=\"http://www.sqlparty.com/wp-content/uploads/2013/10/NN_HA.png\"><img
  class=\"alignnone size-full wp-image-730\" alt=\"NN_HA\" src=\"http://www.sqlparty.com/wp-content/uploads/2013/10/NN_HA.png\"
  width=\"929\" height=\"626\" /></a>\r\n本图引自\"<a href=\"https://issues.apache.org/jira/secure/attachment/12480489/NameNode%20HA_v2_1.pdf\"
  target=\"_blank\">High \PAvailability \Pfor \Pthe \PHDFS \PNamenode</a>\"。\r\n\r\n从上述的架构图，我们可以看到搭建NameNode
  HA应关注的地方：\r\n\r\n"
wordpress_id: 729
wordpress_url: http://www.sqlparty.com/?p=729
date: '2013-10-12 11:29:04 +0800'
date_gmt: '2013-10-12 03:29:04 +0800'
categories:
- 大数据
tags:
- Hadoop
- hdfs
- ha
---
<p>CDH4以后，支持NameNode做HA，这样避免了单点故障(single point of failure, SPOF)的问题，保障了整个集群的整体可用性。</p>
<h2>原理</h2><br />
在HA出现之前，Secondary NameNode看上去似乎起着保证NameNode高可用的作用，但其实不然，Secondary NameNode只是定期合并fsimage和edits信息，替NameNode整理edits。它不是实时与NameNode保持更新的，所以用它来顶替宕机的NameNode势必会有元数据丢失风险！</p>
<p>NameNode的HA方案，是一个集群中配置两个（当前不能配置多个）NameNode，其中一个处于活跃Active状态，另一个为Standby状态。如果活跃的NameNode宕机或者正常关闭，Standby可以快速切换为Active。</p>
<p>NameNode HA的架构图：</p>
<p><a href="http://www.sqlparty.com/wp-content/uploads/2013/10/NN_HA.png"><img class="alignnone size-full wp-image-730" alt="NN_HA" src="http://www.sqlparty.com/wp-content/uploads/2013/10/NN_HA.png" width="929" height="626" /></a><br />
本图引自"<a href="https://issues.apache.org/jira/secure/attachment/12480489/NameNode%20HA_v2_1.pdf" target="_blank">High  Availability  for  the  HDFS  Namenode</a>"。</p>
<p>从上述的架构图，我们可以看到搭建NameNode HA应关注的地方：</p>
<p><!--more--></p>
<p><strong>1.确保只有一个NameNode提供服务</strong></p>
<p>Active NameNode与Standby NameNode，同一时刻只应有一个活跃状态、提供服务的NameNode。要保障同一时刻只有一个Active NameNode，需要有一定的隔离(fencing)机制。</p>
<p><strong>2.Standby NameNode与Active NameNode元数据同步机制</strong></p>
<p>Standby NameNode应能够实时访问Active NameNode的edits日志，所以可靠日志的共享机制非常重要。</p>
<p><strong>3.故障检测</strong></p>
<p>如果Active NN失效、宕机，获取其依赖的OS、硬件出现故障，如何进行检测？这里就需要FailoverController组件。</p>
<p><strong>4.自动转移</strong></p>
<p><strong></strong>故障检测的目的就是期望能够快速的自动切换，使得故障不影响NameNode的对外服务。而自动检测这里依赖于FailoverController组件与ZooKeeper集群的交互。</p>
<p><strong>5.Standby NameNode与Active NameNode之间BLOCK块同步</strong></p>
<p>NameNode启动时将所有HDFS元数据信息从fsimage和edits中导入内存，元数据更新操作写入edits。但是数据块BLOCK信息不存储到文件，是NN启动后与DN实时交互获取的。如果发生故障切换，而新上任的NameNode没有实时的BLOCK信息，依然无法完成服务。所以要求DN的BLOCK信息要同时汇报给所有NameNode。</p>
<p>在以上关注点下，更细一步，则需要回答下述问题：</p>
<p>一、NameNode的HA环境，对外的统一访问接口是怎样的？<br />
二、可靠日志的共享如何实现？<br />
三、如何设置隔离机制fencing，来保证同一时刻只有一个NameNode提供服务？<br />
四、自动转移机制如何实现？<br />
五、有哪些NameNode HA管理命令？</p>
<p>以下结合基本配置回答上述的问题。</p>
<h2>一、NameNode的HA环境，对外的统一访问接口是怎样的？</h2><br />
HA集群使用<strong>NameService ID</strong>来唯一的标识一个单独的HDFS实例，后台支持多个HA NameNode节点。另外，添加了新的抽象概念NameNode ID，标示一个NameNode。为了支持在一个配置文件中配置所有的NameNode，相应的配置参数使用<strong>NameService ID</strong>与<strong>NameNode</strong>做前缀。</p>
<p><span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>fs.defaultFS</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>hdfs://mycluster</value>&nbsp;</span> #这里mycluster即为自定义的NameServiceID<br />
<span style="color: #0000ff;"></property></span></p>
<p>要配置HA NameNodes，你必须在hdfs-site.xml中添加几个配置选项。</p>
<p><strong>1.定义nameservice id</strong></p>
<p><span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.nameservices</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>mycluster</value></span><br />
<span style="color: #0000ff;"></property></span></p>
<p><strong>2.定义每个NameNode节点的RPC地址</strong></p>
<p>配置dfs.rpc-address.[nameservice ID].[name node id]，来定义每个NameNode的RPC地址。Client就是通过RPC地址与NameNode进行交互的。</p>
<p><span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.namenode.rpc-address.mycluster.nn1</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>machine1.example.com:8020</value></span><br />
<span style="color: #0000ff;"></property></span><br />
<span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.namenode.rpc-address.mycluster.nn2</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>machine2.example.com:8020</value></span><br />
<span style="color: #0000ff;"></property></span></p>
<p><strong>3.配置每个NameNode节点的HTTP地址</strong></p>
<p><span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.namenode.http-address.mycluster.nn1</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>machine1.example.com:50070</value></span><br />
<span style="color: #0000ff;"></property></span><br />
<span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.namenode.http-address.mycluster.nn2</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>machine2.example.com:50070</value></span><br />
<span style="color: #0000ff;"></property></span></p>
<p><strong>4.客户端访问Active NameNode</strong></p>
<p>dfs.client.failover.proxy.provider.[nameservice ID] 设置了HDFS客户端用来联系Active NameNode的Java 类。当前系统只包含一个实现org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider。</p>
<p><span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp; <name>dfs.client.failover.proxy.provider.mycluster</name></span><br />
<span style="color: #0000ff;">&nbsp;&nbsp;&nbsp; <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value></span><br />
<span style="color: #0000ff;"></property></span></p>
<p>如上的配置就是配置NameNode HA所需的最基础部分，也是客户端访问NameNode HA的必要配置。</p>
<h2>二、可靠日志的共享如何实现？</h2><br />
这个其实是HA的核心部分，如何使得Standby与Active NameNode之间可靠的快速同步变化信息，至关重要。方案有多种多样， Linux HA, VMware FT, shared NAS+NFS, BookKeeper, QJM/Quorum Journal Manager, BackupNode等等。目前普遍采用的是shard NAS+NFS，因为简单易用，但是需要提供一个HA的共享存储设备。</p>
<p>CDH4除了支持shard NAS+NFS外还支持Quorum-based Storage方案，前者添加了对NFS的依赖，而这又可能成为故障的单点，不推荐。</p>
<p>这里只介绍使用<strong>Quorum-based Storage</strong>的方案。</p>
<p>这个方案采用的是QJM/Qurom Journal Manager，这是一个基于Paxos算法实现的HDFS HA方案。其原理与ZooKeeper集群的原理相似，都是基于写入大多数节点成功即认为成功，数据不会丢失。假设有2N+1台主机，那么这个算法能够容忍最多N台机器挂掉。具体算法可见"<a href="http://en.wikipedia.org/wiki/Paxos_(computer_science)" target="_blank">Paxos (computer science)</a>"。</p>
<p>用QJM的方式来实现HA的主要好处有：</p>
<ol>
<li>不需要配置额外存储，降低了复杂度和维护成本；</li>
<li>针对日志共享存储不需要再单独配置fencing，因为QJM本身内置了fencing的功能；</li>
<li>可用性高，能够容忍少量机器宕机；</li>
<li>QJM中存储日志的JournalNode不会因为其中一台的延迟而影响整体的延迟，而且也不会因为JournalNode的数量增多而 影响性能（因为NN向JournalNode发送日志是并行的）。</li><br />
</ol><br />
要使用QJM需要进行如下配置：</p>
<p><strong>1.配置JournalNodes的日志目录（端口）</strong></p>
<p>dfs.namenode.shared.edits.dir是共享存储目录的位置，记录日志信息，Active NameNode会向其写入，而Standby NameNode会读取这个目录。尽管必须指定多个JournalNode地址，但是只需要写在一个URI。格式为：</p>
<p>qjournal://<host1:port1>;<host2:port2>;<host3:port3>/<journalId></p>
<p>journalId对于nameservice来说是唯一的，推荐直接使用nameservice id，虽然不是硬性要求。</p>
<p>例如：</p>
<p><span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.namenode.shared.edits.dir</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>qjournal://node1.example.com:8485;node2.example.com:8485;node3.example.com:8485/mycluster</value></span><br />
<span style="color: #0000ff;"></property></span></p>
<p><strong>2.JournalNode daemon保存本地信息的路径</strong></p>
<p>每个JournalNode机器，配置一个完整路径来保存日志与其自身信息。每个JournalNode只使用一个单独的path。</p>
<p><span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.journalnode.edits.dir</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>/data/1/dfs/jn</value></span><br />
<span style="color: #0000ff;"></property></span></p>
<h2>三.如何设置隔离机制fencing，来保证同一时刻只有一个NameNode提供服务？</h2><br />
涉及到3类的fencing：</p>
<p><strong>1）edits日志共享存储的fencing， 确保只有一个NN可以写入；</strong></p>
<p>这部分由存储edits日志的存储QJM来保证，可以避免元数据因同时多写而损坏，无需额外配置。</p>
<p>NameNode的fencing：</p>
<p><strong>2）客户端fencing，确保只有一个NN可以响应客户端的请求；</strong><br />
<strong>3）DataNode fencing，确保只有一个NN可以向DN发命令，譬如删除块，复制块，等等。</strong></p>
<p>要保证这两点，则需要配置在新NameNode启用前，确保已经不再对外服务（隔离fencing就是干这事），下面看看具体如何配置故障转移的防范措施fencing：</p>
<p>虽然使用Quorum-based Storage只允许一个NameNode写JournalNodes，不会有元数据因同时多写而损坏的情况，但是在故障转移过程中，依然有可能之前的NameNode还在提供读服务（只有试图写入时才会shutdown），而读到的可能是过时的数据。所以还是有必要配置一些防范措施。如果你不选择，也还是必须配置一下这个参数，如shell(/bin/true)。</p>
<p>系统自带了两个防范方法：</p>
<p>1) sshfence&mdash;&mdash;免密码SSH到Active NameNode，杀死进程。另外必须要配置dfs.ha.fencing.ssh.private-key-files，要求运行NameNode的用户可访问该文件。</p>
<p><span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.ha.fencing.methods</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>sshfence</value></span><br />
<span style="color: #0000ff;"></property></span><br />
<span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.ha.fencing.ssh.private-key-files</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>/home/exampleuser/.ssh/id_rsa</value></span><br />
<span style="color: #0000ff;"></property></span></p>
<p><strong>2) shell&mdash;&mdash;运行任意shell命令</strong></p>
<p><span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.ha.fencing.methods</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>shell(/path/to/my/script.sh arg1 arg2 ...)</value></span><br />
<span style="color: #0000ff;"></property></span></p>
<p>括号()之间的内容会直接传递给一个bash shell。这个shell环境中可以访问到所有当前的Hadoop配置变量，使用'_'来代替'.'号。还有一些目标节点相关的变量可用，如$target_host,target_nameserviceid等。</p>
<p>一个配置的示例：</p>
<p><span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.ha.fencing.methods</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>shell(/path/to/my/script.sh --nameservice=$target_nameserviceid $target_host:$target_port)</value></span><br />
<span style="color: #0000ff;"></property></span></p>
<p>如果shell命令的返回代码是0，那么说明fencing操作成功。</p>
<h2>四、自动转移机制如何实现？</h2><br />
有了上面的fencing机制，我们可以通过NameNode HA管理命令手动触发故障转移。下一步需要考虑自动故障转移机制，这样才能尽量保障服务的可用。要实现自动故障切换，依赖于ZooKeeper，所以需要一个ZooKeeper quorum，和ZKFailoverController进程（简称ZKFC）。</p>
<p>主要使用到了ZooKeeper的如下功能：</p>
<ol>
<li>故障检测Failure detection：每个NameNode节点通过ZKFC组件在ZooKeeper中维护一个会话，如果该主机宕机，那么ZooKeeper会检测到过期，这样就触发通知另一个节点做故障切换。</li>
<li>活跃NameNode选举：ZooKeeper提供了一个简单的机制，来选出一个活跃节点。如果当前活跃NameNode宕机，另一个节点可以获取ZooKeeper中一个排他锁，来表明其成为活跃节点。</li><br />
</ol><br />
ZKFailoverController(ZKFC)是一个新的组件：它是ZooKeeper的一个客户端，监控和管理NameNode的状态。每个NameNode节点上都运行一个ZKFC。ZKFC负责：</p>
<ol>
<li>健康检查：通过健康检查的命令定期检查本地的NameNode。如果检测到不健康的状态，则标示该节点不健康！</li>
<li>ZooKeeper会话管理：如果本地NameNode健康，那么ZKFC会保持一个ZooKeeper会话；如果它是活跃的，ZKFC还会维持一个临时的znode锁。如果会话失效，则该znode会自动删除。</li>
<li>选举：如果ZKFC检测到NameNode健康，而ZooKeeper中的锁znode不存在，则会尝试获取这个锁。成功则选举成功，负责进行故障切换。故障切换与手动操作类似：先fenced，然后升级为活跃状态。</li><br />
</ol><br />
了解原理后，进行自动切换的配置，在hdfs-site.xml文件中添加：</p>
<p><span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>dfs.ha.automatic-failover.enabled</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>true</value></span><br />
<span style="color: #0000ff;"></property></span><br />
<span style="color: #0000ff;">
<property></span><br />
<span style="color: #0000ff;">&nbsp; <name>ha.zookeeper.quorum</name></span><br />
<span style="color: #0000ff;">&nbsp; <value>zk1.example.com:2181,zk2.example.com:2181,zk3.example.com:2181</value></span><br />
<span style="color: #0000ff;"></property></span></p>
<p>ZooKeeper中的HA状态需要先进行初始化。在完成上述配置后，进行如下初始化，当然要求ZooKeeper集群已运行：</p>
<p><span style="color: #0000ff;">shell> hdfs zkfc -formatZK</span> #在任意一个NameNode上运行即可。会创建一个znode用于自动故障转移。</p>
<p>注意：</p>
<ol>
<li>ZKFC和NameNode两个服务的启动顺序没有影响，谁先启动都可以。</li>
<li>配置自动切换后，应添加对ZKFC的监控。有些情况下的ZooKeeper异常会导致ZKFC异常退出，应该重启之以便保证自动切换机制的可用。</li>
<li>如果ZooKeeper集群失败，并不会导致HDFS集群不可用，只是不能触发自动故障切换而已。ZooKeeper集群恢复正常后，自动切换机制可以恢复。</li><br />
</ol></p>
<h2>五、有哪些NameNode HA管理命令？</h2><br />
配置了NameNode HA后，添加了额外的管理命令hdfs haadmin。这里介绍一些重要的子命令。要查看具体子命令用法，则hdfs haadmin -help <command>。</command></p>
<ul>
<li>-failover [--forcefence] [--forceactive] 手动切换，从第一个NameNode切换到第二个NameNode。如果前一个是Standby状态，则命令仅仅将第二个切换为Active，不会报错。如果前一个是Active状态，则先尝试将其转换为Standby状态，不行则使用dfs.ha.fencing.methods进行关闭操作。只有确保前一处理成功后才会将后一转换为Active。</li>
<li>-getServiceState 查看指定NameNode是Active还是Standby。</li>
<li>-checkHealth 检查指定NameNode的健康状况，返回0则为健康。注：这个选项目前暂未完全实现，除非NameNode完全停止，否则都是返回0。</li><br />
</ul><br />
而管理命令hadoop dfsadmin在HA环境中，可以使用-fs选项来指定特定的NameNode的RPC。如hadoop dfs admin -fs server1:2281 xxx。</p>
<h2>其他补充</h2><br />
<strong>1.格式化HDFS</strong></p>
<p>在HDFS启用前，namenode必须格式化！格式化只需要执行一次，在NameNode中创建文件系统元数据。</p>
<p>格式化命令必须被设定为HDFS super用户的OS用户进行操作，在namenode主机上，而且前提必须是dfs.name.dir目录必须已存在。</p>
<p>执行完毕后，会创建空的fsimage文件和edit log，以及随机生成的storage ID。DataNode连接到NameNode后会获取到这个storage ID，这样以后就拒绝连接到其他namenode。</p>
<p>如果要重新格式化namenode，那么必须删除所有datanode的数据，因为其保护了storage ID信息。</p>
<p>注意：格式化NameNode后，对应DataNode的数据也将永久不可用!所以，多练习！谨慎！</p>
<p>格式化操作：<span style="color: #0000ff;">shell> sudo -u hdfs hadoop namenode -format</span></p>
<p>当Active NameNode完成格式化后，StandBy NameNode不能通过相同的方式进行格式化，因为这样会导致出现不一致的namespace id，使得DataNode不能同时与两个NameNode交互。StandBy NameNode需要与Active NameNode进行同步。</p>
<p>同步可以通过命令：<span style="color: #0000ff;">sudo -u hdfs hadoop namenode -bootstrapStandby</span></p>
<p>也可以通过直接复制Active NameNode下的完整内容，如rsync -av命令。</p>
<p><strong>2.实际搭建</strong></p>
<p>本文主要讲解NameNode HA的原理以及相关的一些基本配置，具体搭建案例参见&ldquo;<a title="HDFS环境搭建-NameNode HA搭建实录" href="http://www.sqlparty.com/hdfs%e7%8e%af%e5%a2%83%e6%90%ad%e5%bb%ba-namenode-ha%e6%90%ad%e5%bb%ba%e5%ae%9e%e5%bd%95/" target="_blank">NameNode HA搭建实录</a>&rdquo;。</p>
<p>参考：</p>
<p><a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-High-Availability-Guide/cdh4hag_topic_2_3.html" target="_blank">http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH4-High-Availability-Guide/cdh4hag_topic_2_3.html</a><br />
<a href="http://www.infoq.com/cn/articles/hadoop-2-0-namenode-ha-federation-practice-zh" target="_blank">http://www.infoq.com/cn/articles/hadoop-2-0-namenode-ha-federation-practice-zh</a><br />
<a href="http://yanbohappy.sinaapp.com/?p=205" target="_blank">http://yanbohappy.sinaapp.com/?p=205</a></p>
