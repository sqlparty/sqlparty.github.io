---
layout: post
status: publish
published: true
title: Hive远程元数据服务Thrift Metastore搭建与管理
author:
  display_name: super
  login: super
  email: sqlparty@gmail.com
  url: ''
author_login: super
author_email: sqlparty@gmail.com
excerpt: "Hive中，只有metastore组件是Hadoop没有的。metastore保存&#47;访问Hive的表定义与分区信息。\r\n\r\n本文介绍metastore使用MySQL作为后台存储，而且创建独立服务而不是与hive客户端绑定运行在同一个JVM中。\r\n\r\n其服务的架构类似如下：\r\n\r\n<a
  href=\"http:&#47;&#47;www.sqlparty.com&#47;wp-content&#47;uploads&#47;2013&#47;09&#47;metastore.png\"><img
  class=\"alignnone size-full wp-image-677\" alt=\"metastore\" src=\"http:&#47;&#47;www.sqlparty.com&#47;wp-content&#47;uploads&#47;2013&#47;09&#47;metastore.png\"
  width=\"331\" height=\"387\" &#47;><&#47;a>\r\n本图引自<a href=\"http:&#47;&#47;www.cloudera.com&#47;content&#47;cloudera-content&#47;cloudera-docs&#47;CDH4&#47;4.2.0&#47;CDH4-Installation-Guide&#47;cdh4ig_topic_18_4.html\"
  target=\"_blank\"> Cloudera<&#47;a>\r\n\r\n"
wordpress_id: 676
wordpress_url: http://www.sqlparty.com/?p=676
date: '2013-09-15 14:58:08 +0800'
date_gmt: '2013-09-15 06:58:08 +0800'
categories:
- 大数据
tags:
- hive
- thrift
- metastore
---
<p>Hive中，只有metastore组件是Hadoop没有的。metastore保存&#47;访问Hive的表定义与分区信息。</p>
<p>本文介绍metastore使用MySQL作为后台存储，而且创建独立服务而不是与hive客户端绑定运行在同一个JVM中。</p>
<p>其服务的架构类似如下：</p>
<p><a href="http:&#47;&#47;www.sqlparty.com&#47;wp-content&#47;uploads&#47;2013&#47;09&#47;metastore.png"><img class="alignnone size-full wp-image-677" alt="metastore" src="http:&#47;&#47;www.sqlparty.com&#47;wp-content&#47;uploads&#47;2013&#47;09&#47;metastore.png" width="331" height="387" &#47;><&#47;a><br />
本图引自<a href="http:&#47;&#47;www.cloudera.com&#47;content&#47;cloudera-content&#47;cloudera-docs&#47;CDH4&#47;4.2.0&#47;CDH4-Installation-Guide&#47;cdh4ig_topic_18_4.html" target="_blank"> Cloudera<&#47;a></p>
<p><!--more--></p>
<h2>一、后台数据库选择<&#47;h2><br />
Hive默认的内置Derby SQL服务器，只提供有限的、单进程的存储操作，如不能同时运行两个Hive查询。任何JDBC兼容的数据库都可以作为metastore的库，对于生产环境、Hadoop集群，推荐使用MySQL。</p>
<p><span style="color: #0000ff;">注：虽然metastore中数据量相对其后台存储的数据而言非常小，但是其是单点（Single Point of Failure），所以保证其高可用性非常重要！针对MySQL而言，可以做复制！<&#47;span></p>
<p>要将Hive的元数据保存到MySQL中，应该要有：</p>
<h3>1.适当的配置<&#47;h3><br />
分别配置jdbc url，驱动，用户名，密码，使用如下选项：</p>
<ul>
<li>javax.jdo.option.ConnectionURL<&#47;li>
<li>javax.jdo.option.ConnectionDriverName<&#47;li>
<li>javax.jdo.option.ConnectionUserName<&#47;li>
<li>javax.jdo.option.ConnectionPassword<&#47;li><br />
<&#47;ul><br />
具体含义见下文服务端配置。</p>
<h3>2.正确的表结构<&#47;h3><br />
Hive在首次连接时如果检测到数据库或表结构不存在，会自动创建，所以应赋予其合适的权限。</p>
<p style="padding-left: 30px;">关于表结构，在实践过程中遇到问题如下：<br />
我们的MySQL服务器设置是utf8&#47;utf8_bin。这样的设置下，hive自行创建表时会报错<br />
<em>com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes)<&#47;em><br />
<em> FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask<&#47;em><br />
经查，其执行如下SQL时报的如下错：<br />
<span style="color: #0000ff;">CREATE TABLE "TABLE_PARAMS"<&#47;span><br />
<span style="color: #0000ff;"> (<&#47;span><br />
<span style="color: #0000ff;"> "TBL_ID" BIGINT NOT NULL,<&#47;span><br />
<span style="color: #0000ff;"> "PARAM_KEY" VARCHAR(256 ) BINARY NOT NULL,<&#47;span><br />
<span style="color: #0000ff;"> "PARAM_VALUE" VARCHAR(4000 ) BINARY NULL,<&#47;span><br />
<span style="color: #0000ff;"> PRIMARY KEY ("TBL_ID" ,"PARAM_KEY" )<&#47;span><br />
<span style="color: #0000ff;"> ) ENGINE=INNODB<&#47;span><br />
MySQL中对索引的长度有限制，最大不能超过1000字节，而InnoDB引擎的索引每列最大不能超过767字节。由于MySQL的UTF8字符集中每个字符占3个字节，而这里PARAM_KEY最大长度3*256=<br />
768）超过了767的限制，所以有此报错信息。<&#47;p></p>
<p style="padding-left: 30px;">这里采取的解决方案的是：<br />
根据&#47;usr&#47;lib&#47;hive&#47;scripts&#47;metastore&#47;upgrade&#47;mysql目录下的脚本信息，了解到其原生的字符集支持是latin1，所以这里手动创建数据库，指定字符集。<br />
<span style="color: #0000ff;">CREATE DATABASE metastore DEFAULT CHARACTER SET latin1<&#47;span><&#47;p><br />
也可以手动创建好所需的表:<br />
<span style="color: #0000ff;">mysql> CREATE DATABASE metastore;<&#47;span><br />
<span style="color: #0000ff;"> mysql> USE metastore;<&#47;span><br />
<span style="color: #0000ff;"> mysql> SOURCE &#47;usr&#47;lib&#47;hive&#47;scripts&#47;metastore&#47;upgrade&#47;mysql&#47;hive-schema-0.10.0.mysql.sql<&#47;span></p>
<h2>二、Metastore服务配置<&#47;h2><br />
这里搭建了Metastore作为远程服务，这样又划分了服务端&#47;客户端两端的配置</p>
<h2>1.服务端<&#47;h2><br />
服务端的责任是接受客户端的连接响应，另与后台数据库进行交互。有如下配置项：</p>
<div>
<table class="table">
<tbody>
<tr>
<th>配置项<&#47;th></p>
<th>配置值<&#47;th></p>
<th>注释<&#47;th><br />
<&#47;tr></p>
<tr>
<td>javax.jdo.option.ConnectionURL<&#47;td></p>
<td>jdbc:mysql:&#47;&#47;<host name>&#47;<database name>?createDatabaseIfNotExist=true<&#47;td></p>
<td>MySQL的路径<&#47;td><br />
<&#47;tr></p>
<tr>
<td>javax.jdo.option.ConnectionDriverName<&#47;td></p>
<td>com.mysql.jdbc.Driver<&#47;td></p>
<td>MySQL JDBC驱动<&#47;td><br />
<&#47;tr></p>
<tr>
<td>javax.jdo.option.ConnectionUserName<&#47;td></p>
<td><user name><&#47;td></p>
<td>MySQL用户名<&#47;td><br />
<&#47;tr></p>
<tr>
<td>javax.jdo.option.ConnectionPassword<&#47;td></p>
<td>
<password><&#47;td></p>
<td>MySQL密码<&#47;td><br />
<&#47;tr></p>
<tr>
<td>hive.metastore.warehouse.dir<&#47;td></p>
<td><base path><&#47;td></p>
<td>Hive表的默认路径，结合Hadoop的fs.defaultFS确定是存储在HDFS还是local。<&#47;td><br />
<&#47;tr></p>
<tr>
<td>datanucleus.autoStartMechanism<&#47;td></p>
<td>SchemaTable<&#47;td></p>
<td>SchemaTable选项设置DataNucleus启动时读入NUCLEUS_TABLES的内容(包括需要载入的类、相应的表、类型、版本等信息)，这些信息有助于DataNucleus进行结构的一致性检测。注：DataNucleus是Hive用来与RDBMS进行交互的组件。<&#47;td><br />
<&#47;tr><br />
<&#47;tbody><br />
<&#47;table><br />
<&#47;div><br />
示例：</p>
<property>
<name>hive.metastore.warehouse.dir<&#47;name><br />
<value>&#47;hive<&#47;value><br />
<description>Local or HDFS directory where Hive keeps table contents, according to fs.default.name.<&#47;description><br />
<&#47;property></p>
<property>
<name>javax.jdo.option.ConnectionURL<&#47;name><br />
<value>jdbc:mysql:&#47;&#47;192.168.10.118:3306&#47;bmmetastoredb?createDatabaseIfNotExist=true<&#47;value><br />
<&#47;property></p>
<property>
<name>javax.jdo.option.ConnectionDriverName<&#47;name><br />
<value>com.mysql.jdbc.Driver<&#47;value><br />
<&#47;property></p>
<property>
<name>javax.jdo.option.ConnectionUserName<&#47;name><br />
<value>user1<&#47;value><br />
<&#47;property></p>
<property>
<name>javax.jdo.option.ConnectionPassword<&#47;name><br />
<value>pass1<&#47;value><br />
<&#47;property><br />
<!-- datanucleus --></p>
<property>
<name>datanucleus.autoStartMechanism<&#47;name><br />
<value>SchemaTable<&#47;value><br />
<&#47;property></p>
<p>MySQL的连接驱动下载后，将mysql-connector-java-5.1.26-bin.jar置于&#47;usr&#47;lib&#47;hive&#47;lib&#47;目录下，这样连接mysql所需要的jdbc驱动也能被找到了。</p>
<p>要配置MetaStore使用HDFS HA，则需要分配&#47;etc&#47;hadoop&#47;conf下的core-site.xml和hdfs-site.xml文件。如下：<br />
<!--  core-site.xml --><!--&nbsp; core-site.xml --></p>
<property>
<name>fs.defaultFS<&#47;name><br />
<value>hdfs:&#47;&#47;myha&#47;<&#47;value><br />
<&#47;property></p>
<p><!-- hdfs-site.xml --></p>
<property>
<name>dfs.nameservices<&#47;name><br />
<value>myha<&#47;value><br />
<&#47;property></p>
<property>
<name>dfs.ha.namenodes.myha<&#47;name><br />
<value>nn1,nn2<&#47;value><br />
<&#47;property></p>
<property>
<name>dfs.namenode.rpc-address.myha.nn1<&#47;name><br />
<value>CHBM220:8020<&#47;value><br />
<&#47;property></p>
<property>
<name>dfs.namenode.rpc-address.myha.nn2<&#47;name><br />
<value>CHBM221:8020<&#47;value><br />
<&#47;property></p>
<property>
<name>dfs.client.failover.proxy.provider.myha<&#47;name><br />
<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<&#47;value><br />
<&#47;property><br />
<span style="color: #0000ff;">注意：dfs.client.failover.proxy.provider.myha的配置不可或缺！<&#47;span></p>
<h2>2.客户端<&#47;h2><br />
客户端要连接到远程的Metastore服务，需要使用如下配置项。</p>
<table class="table">
<tbody>
<tr>
<th>配置项<&#47;th></p>
<th>配置值<&#47;th></p>
<th>注释<&#47;th><br />
<&#47;tr></p>
<tr>
<td>hive.metastore.uris<&#47;td></p>
<td>thrift:&#47;&#47;<host_name>:
<port><&#47;td></p>
<td>设置远程thrift metastore server的访问路径。默认端口是：9083<&#47;td><br />
<&#47;tr></p>
<tr>
<td>hive.metastore.local<&#47;td></p>
<td>false<&#47;td></p>
<td>因为使用远程metastore服务，所以这里设为false。<&#47;td><br />
<&#47;tr><br />
<&#47;tbody><br />
<&#47;table><br />
示例：</p>
<property>
<name>hive.metastore.uris<&#47;name><br />
<value>thrift:&#47;&#47;CHBM224:9083<&#47;value><br />
<&#47;property></p>
<property>
<name>hive.metastore.local<&#47;name><br />
<value>false<&#47;value><br />
<&#47;property></p>
<h2>三、Metastore管理<&#47;h2><br />
<a title="创建cdh4本地Yum仓库" href="http:&#47;&#47;www.sqlparty.com&#47;%e5%88%9b%e5%bb%bacdh4%e6%9c%ac%e5%9c%b0yum%e4%bb%93%e5%ba%93&#47;" target="_blank">CDH4的yum源<&#47;a>中包括了hive-metastore的管理包，安装。<br />
<span style="color: #0000ff;">shell> yum install hive-metastore<&#47;span><br />
安装后，就可以使用其管理脚本进行管理，其已经注册为服务。</p>
<h3>1.服务端启动<&#47;h3><br />
<span style="color: #0000ff;">shell> service hive-metastore start<&#47;span><br />
不指定端口的话模式使用9083端口，如果要指定端口，则在&#47;etc&#47;default&#47;hive-metastore中加入：<br />
export PORT=9084<br />
启动脚本会自动读取该文件中的配置。</p>
<h3>2.服务端是否活跃<&#47;h3><br />
<span style="color: #0000ff;">shell> service hive-metastore status<&#47;span><br />
<span style="color: #0000ff;"> Hive Metastore is running [ OK ]<&#47;span></p>
<h3>3.服务端关闭<&#47;h3><br />
<span style="color: #0000ff;">shell> service hive-metastore stop<&#47;span></p>
<p>管理脚本照顾到了一些我们容易忽略的地方，使得我们操作更方便。如果不使用所安装的hive-metastore脚本进行管理，而是进行手动配置的话，可能需要考虑到如下一些情况：</p>
<p style="padding-left: 30px;">1.服务端启动<br />
Metastore服务启动，不指定端口的话模式使用9083端口：<br />
shell> sudo -u hive hive --service metastore<&#47;p></p>
<p style="padding-left: 30px;">权限问题：<br />
这里注意启动服务使用的用户，如果root用户在hdfs上没有创建文件等权限，则会影响建表等操作。这里使用hive用户（sudo -u hive）。<br />
当然首先应确保，hive用户应拥有HDFS上&#47;hive（即hive.metastore.warehouse.dir指定目录）的创建或者读写权限。<br />
如果没有的话，可以使用HDFS的supergroup用户进行如下方式操作：<br />
shell>hadoop fs -mkdir &#47;hive<br />
shell>hadoop fs -chown hive:hdfs &#47;hive<br />
这样后续hive用户就具有完全的控制权限。客户端远程连接服务后，执行的权限就是服务启动所使用的用户权限。<&#47;p></p>
<p style="padding-left: 30px;">可以使用-p指定端口：<br />
shell> hive --service metastore -p
<port_num><&#47;p></p>
<p style="padding-left: 30px;">作为服务的话，最好启用后台运行，如：<br />
shell> hive --service metastore -p 9084 &amp;<&#47;p></p>
<p style="padding-left: 30px;">2.查询是否活动<br />
shell> netstat -tlnp | grep 9083<br />
tcp 0 0 0.0.0.0:9083 0.0.0.0:* LISTEN 7008&#47;java<br />
如上有信息返回即是正常。<&#47;p></p>
<p style="padding-left: 30px;">3.关闭服务<br />
shell> kill 7008 #指定查询到的进程的pid。<&#47;p><br />
另外例如开机启动，hive-metastore服务已经整合了这个功能。</p>
<p>参考：<br />
<a href="https:&#47;&#47;cwiki.apache.org&#47;confluence&#47;display&#47;Hive&#47;AdminManual+MetastoreAdmin" target="_blank">https:&#47;&#47;cwiki.apache.org&#47;confluence&#47;display&#47;Hive&#47;AdminManual+MetastoreAdmin<&#47;a><br />
《Programming Hive》</p>
